{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = r\"C:\\Users\\rose_\\OneDrive\\Desktop\\DA_Project\\DA_Project\"   ## use your direcitory (.../data)\n",
    "home2 = home + '/data'  ## /data/data path can get access to all raw data (tweets, followings)\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "### Set an API\n",
    "with open(\"twitter_key&secret key.txt\") as f:\n",
    "    api_key = f.read().split(\"\\n\")\n",
    "consumer_key = api_key[0]\n",
    "consumer_secret = api_key[1]\n",
    "access_token = api_key[2]\n",
    "access_token_secret = api_key[3]\n",
    "\n",
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Setting wait_on_rate_limit and wait_on_rate_limit_notify to True makes the API object print a message and wait if the rate limit is exceeded\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(id_list):\n",
    "    from tqdm import tqdm_notebook\n",
    "    # Read tweet into dictionary\n",
    "    import os\n",
    "    path = r\"C:\\Users\\rose_\\OneDrive\\Desktop\\DA_Project\\DA_Project\\data\" \n",
    "    os.chdir(path)\n",
    "    tweet = dict()\n",
    "    description = dict()\n",
    "    for i in tqdm_notebook(id_list):\n",
    "        path2 = path +f\"/{i}\"\n",
    "        files = os.listdir(path2)\n",
    "        for file in files:\n",
    "            if file=='description.txt': \n",
    "                f = open(path2+\"/\"+file); \n",
    "                iter_f = iter(f);\n",
    "                str = \"\"\n",
    "                for line in iter_f: \n",
    "                    str = str + line\n",
    "                description[i]=str\n",
    "            elif file=='tweet.txt': \n",
    "                f = open(path2+\"/\"+file); \n",
    "                iter_f = iter(f); \n",
    "                str = \"\"\n",
    "                for line in iter_f: \n",
    "                    str = str + line\n",
    "                tweet[i]=str\n",
    "\n",
    "    # Text cleaning, get rid of non-English text\n",
    "    from langdetect import detect\n",
    "    import re\n",
    "    import demoji\n",
    "\n",
    "    tweet_cleaned=dict()\n",
    "    description_cleaned=dict()\n",
    "\n",
    "    for i in tqdm_notebook(tweet.keys()):\n",
    "        string_=tweet[i]\n",
    "        # remove @name\n",
    "        string_=re.sub(r'RT @\\S+', '', string_, count=0, flags=0)\n",
    "\n",
    "        # remove 'https://t.co/\\w+'\n",
    "        string_=re.sub(r'https://t.co/\\w+', '', string_, count=0, flags=0)\n",
    "\n",
    "        #count the number of different emojis of each user\n",
    "        count=dict()\n",
    "        try:\n",
    "            for j in demoji.findall(tweet[i]).keys():\n",
    "                count[j]=len(re.findall(j,tweet[i]))\n",
    "        except:\n",
    "            count['None']='No emoji'\n",
    "\n",
    "        # remove emoji\n",
    "        string_=demoji.replace(string_,'')\n",
    "\n",
    "        # remove other languages and user with empty text\n",
    "        if not string_=='':\n",
    "            try:\n",
    "                if detect(string_)=='en' and string_!=None:\n",
    "                    # clean again\n",
    "                    string_=re.sub(r'[^A-Za-z0-9 _.,!\\\"\\'|]+',\" \",string_)\n",
    "                    tweet_cleaned[i]=string_\n",
    "                    description_cleaned[i]=description[i]\n",
    "            except:\n",
    "                print(\"We can not detect the language\")\n",
    "    keys = [int(i) for i in list(tweet_cleaned.keys())]\n",
    "    \n",
    "    return tweet, tweet_cleaned, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('full_data.csv')\n",
    "valid_id = list(valid_data['id'])\n",
    "tweet, tweet_cleaned, keys = text_preprocessing(valid_id)\n",
    "id_cleaned = pd.DataFrame(keys,columns=[\"id\"])\n",
    "# Delete non-english users in dataframe\n",
    "valid_data = valid_data.merge(id_cleaned,how=\"right\",on=\"id\")\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the all the emoji for each one\n",
    "import demoji\n",
    "from tqdm import tqdm_notebook\n",
    "emoji={}\n",
    "for i in tqdm_notebook(tweet.keys()):\n",
    "    emoji[i]=demoji.findall(tweet[i])\n",
    " \n",
    "emoji_count=dict()\n",
    "import re\n",
    "for i in tqdm_notebook(tweet.keys()):\n",
    "    count=list()\n",
    "    try:\n",
    "        for j in emoji[i].keys():\n",
    "            count.append([j,len(re.findall(j,tweet[i]))])\n",
    "    except:\n",
    "        pass\n",
    "    emoji_count[i]=count\n",
    "emoji_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
